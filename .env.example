# ======== Netty Chat Configuration ========
# Provider Selection: "ollama", "openai", or "deepseek"
LLM_PROVIDER="openai"

# === ChatGPT (OpenAI) ===
LLM_REMOTE_OPENAI_URL="https://api.chatanywhere.org"#"https://free.v36.cm/v1"
LLM_REMOTE_OPENAI_MODEL="gpt-4o-mini"
LLM_REMOTE_OPENAI_API_KEY="[sk-something, put it here]"

# === DeepSeek ===
LLM_REMOTE_URL="https://api.deepseek.com"
LLM_REMOTE_MODEL="deepseek-chat"
LLM_REMOTE_API_KEY="[sk-something, put it here]"

# === Ollama (Local) ===
OLLAMA_BASE_URL="http://localhost:11434/v1"
OLLAMA_MODEL="[the one you use]"


# ======== Cache Configuration ========
# File name for the response cache
KV_NAME="netty-chat.kv"

# ======== Feature Flags ========
# Set to "true" to generate related questions, "false" to disable
RELATED_QUESTIONS="true"

# ======== Legacy Configuration (Optional) ========
# Only used if you revert to older server versions
LLM_USE_CUSTOM_SERVER="true"
LLM_REMOTE_URL="http://localhost:11434/v1"
LLM_REMOTE_API_KEY="ollama"
LLM_MODEL="[the one you use]"

